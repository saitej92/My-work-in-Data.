

---
title: "Tennis Men Data"
author: "Sai teja Gollapinni"
date: "October 31, 2017"
output: html_document
---

```{r}
#Tennis data Internship
#######################################################
##Re using the data to impute Na differently
rm(list = ls(all=T))
#set the working directory 
setwd("C:/Users/saite/Documents/Tennis data Internship/raw data")
getwd()
#read all theGrand slams for year 2013
#1.Aus open-Men followed by women
Ausopen_men= read.csv(file="AusOpen-men-2013.csv", header = T, sep = ",")
#Ausopen_men$Tnmt="Ausopen"
#no of NA's,summary and structure of data-M
sum(is.na(Ausopen_men))
str(Ausopen_men)
summary(Ausopen_men)
#2.French open Men
Fraopen_men=read.csv(file = "FrenchOpen-men-2013.csv", header = T, sep = ",")
#Fraopen_men$Tnmt="Fraopen"
#Na's, summary and structure -M
sum(is.na(Fraopen_men))
str(Fraopen_men)
summary(Fraopen_men)
#3.Wimbledon Men 
Wim_men =read.csv(file = "Wimbledon-men-2013.csv", header = T, sep = ",")
#Wim_men$Tnmt="Wimopen"
#Na's, summary and structure -M
sum(is.na(Wim_men))
str(Wim_men)
summary(Wim_men)
#4. US open men
Usopen_men=read.csv(file = "USOpen-men-2013.csv", header = T, sep = ",")
#Usopen_men$Tnmt="Usopen"
#Na's, summary and structure -M
sum(is.na(Usopen_men))
str(Usopen_men)
summary(Usopen_men)
##convering Result into categorical
Ausopen_men$Result=as.factor(as.character(Ausopen_men$Result))
str(Ausopen_men)

Fraopen_men$Result=as.factor(as.character(Fraopen_men$Result))
str(Fraopen_men)

Wim_men$Result=as.factor(as.character(Wim_men$Result))
str(Wim_men)

Usopen_men$Result=as.factor(as.character(Usopen_men$Result))
str(Usopen_men)
##removing all the Player set attributes 
names(Ausopen_men)
names(Fraopen_men)
names(Wim_men)
names(Usopen_men)
colnames(Ausopen_men)[5]=colnames(Fraopen_men)[5]
colnames(Ausopen_men)[6]=colnames(Fraopen_men)[6]
CombinedMen_1=rbind(Ausopen_men,Fraopen_men)
str(CombinedMen_1)
#combined data 2 
colnames(Usopen_men)[5]=colnames(Wim_men)[5]
colnames(Usopen_men)[6]=colnames(Wim_men)[6]
CombinedMen_2=rbind(Wim_men,Usopen_men)
combineddata_M=rbind(CombinedMen_1,CombinedMen_2)
```
#converting dual to single player data
```{r}
combineddata_M$FSP = (combineddata_M$FSP.1 - combineddata_M$FSP.2)
combineddata_M$FSW = (combineddata_M$FSW.1 - combineddata_M$FSW.2)
combineddata_M$SSP = (combineddata_M$SSP.1 - combineddata_M$SSP.2)
combineddata_M$SSW = (combineddata_M$SSW.1 - combineddata_M$SSW.2)
combineddata_M$ACE = (combineddata_M$ACE.1 - combineddata_M$ACE.2)
combineddata_M$DBF = (combineddata_M$DBF.1 - combineddata_M$DBF.2)
combineddata_M$WNR = (combineddata_M$WNR.1 - combineddata_M$WNR.2)
combineddata_M$UFE = (combineddata_M$UFE.1 - combineddata_M$UFE.2)
combineddata_M$BPC = (combineddata_M$BPC.1 - combineddata_M$BPC.2)
combineddata_M$BPW = (combineddata_M$BPW.1 - combineddata_M$BPW.2)
combineddata_M$NPA = (combineddata_M$NPA.1 - combineddata_M$NPA.2)
combineddata_M$NPW = (combineddata_M$NPW.1 - combineddata_M$NPW.2)
combineddata_M$TPW = (combineddata_M$TPW.1 - combineddata_M$TPW.2)
head(combineddata_M)
combineddata_M=combineddata_M[ , !(colnames(combineddata_M)) %in% (c('Player1','Player2','FNL.1','FNL.2','FSP.1','FSW.1','SSP.1','SSW.1','ACE.1','DBF.1','WNR.1','UFE.1','BPC.1','BPW.1','NPA.1','NPW.1',
 'TPW.1','ST1.1','ST2.1','ST3.1','ST4.1','ST5.1','ST1.2','ST2.2','ST3.2','ST4.2','ST5.2','FSP.2','FSW.2','SSP.2','SSW.2','ACE.2','DBF.2','WNR.2','UFE.2','BPC.2','BPW.2','NPA.2','NPW.2','TPW.2'))]
str(combineddata_M)
summary(combineddata_M)
sum(is.na(combineddata_M))
```
##See the data distribution and outliers 
```{r}
library(car)
scatterplot(combineddata_M$FSP,combineddata_M$SSP)
scatterplot(combineddata_M$FSW,combineddata_M$SSW)
scatterplot(combineddata_M$FSP,combineddata_M$FSW)
scatterplot(combineddata_M$SSP,combineddata_M$SSW)
scatterplot(combineddata_M$FSW,combineddata_M$ACE)
scatterplot(combineddata_M$FSW,combineddata_M$DBF)
scatterplot(combineddata_M$FSW,combineddata_M$UFE)
scatterplot(combineddata_M$FSW,combineddata_M$BPC)
scatterplot(combineddata_M$FSW,combineddata_M$BPW)
scatterplot(combineddata_M$FSW,combineddata_M$NPA)
scatterplot(combineddata_M$FSW,combineddata_M$NPW)
#scatterplot(combineddata_M$Round,combineddata_M$UFE)
```
##Imputation 
####Imputations zero
```{r}
combineddataM_zimp=combineddata_M
combineddataM_zimp[is.na(combineddataM_zimp)]=0
sum(is.na(combineddataM_zimp))
```
##central Imputation
```{r}
library(DMwR)
combineddataM_cimp=centralImputation(data = combineddata_M)
sum(is.na(combineddataM_cimp))
combineddataM_num=combineddata_M[,-c(2)]
combineddataM_cat = data.frame(combineddata_M[,c("Result")])
colnames(combineddataM_cat)="Result"
#combineddataM_cat_2 = data.frame(combineddata_M[,c("Tnmt")])
#colnames(combineddataM_cat_2)="Tmnt"
#combineddataM_cat=cbind(combineddataM_cat_1,combineddataM_cat_2)
colnames(combineddataM_cat)
str(combineddataM_cat)
```
#Knn imp
```{r}
complete.cases(combineddataM_num)
combineddataM_Kimp=knnImputation(data = combineddataM_num, k=3)
sum(is.na(combineddataM_Kimp))
sum(complete.cases(combineddata_M))
colSums(is.na(combineddata_M))
```
#standardize 
```{r}
library(vegan)
combineddataM_range=decostand(combineddataM_num,"range")
summary(combineddataM_num)
combineddataM_stand=decostand(combineddataM_Kimp,"standardize")# zscore method
CombinedMen_final=cbind(combineddataM_cat,combineddataM_stand)
#correlation 
library(corrplot)
combineddataM_fcorr=cor(combineddataM_num)
corrplot(combineddataM_fcorr,method = 'number')
library(car)
#scatterplot(combineddataM_num$DBF, combineddataM_cat_1$Result)
Boxplot(x=combineddataM_cat,y=combineddataM_num$ACE)
```
#splitting the data
#seeding the environment 
```{r}
set.seed(786)
library(caret)
combineddataMtrain_row=createDataPartition(CombinedMen_final$Result, p=0.7,list=F)   
combineddataMtrain_data=CombinedMen_final[combineddataMtrain_row,]
combineddataMtest_data=CombinedMen_final[-combineddataMtrain_row,]
##Build the model-Logistic regression 
summary(combineddataMtrain_data)
combineddataM_logreg= glm(Result~FSW+ACE+UFE+DBF,data = combineddataMtrain_data, family = binomial)
summary(combineddataM_logreg)
##stepAIC
library(MASS)
combineddataM_Lregstep=stepAIC(combineddataM_logreg)
summary(combineddataM_Lregstep)
##Vif
library(car)
combineddataM_vif= vif(combineddataM_Lregstep)

summary(combineddataM_vif)
combineddataM_logreg= glm(Result~FSW+ACE+UFE,data = combineddataMtrain_data, family = binomial)
summary(combineddataM_logreg)
prob_train_combM <- predict(combineddataM_Lregstep, type = "response")
library(ROCR)
pred_combM <- prediction(prob_train_combM, combineddataMtrain_data$Result)

perf_combM<- performance(pred_combM, measure="tpr", x.measure="fpr")
plot(perf_combM, col=rainbow(10), colorize=T, print.cutoffs.at=seq(0,1,0.05))

perf_auc_combM <- performance(pred_combM, measure="auc")
# Access the auc score from the performance object
auc_combM <- perf_auc_combM@y.values[[1]]

print(auc_combM)

prob_train_combM= predict(combineddataM_logreg,combineddataMtrain_data)
pred_train_combM= ifelse(prob_train_combM >0.65,"1","0")
table(pred_train_combM)
confusionMatrix(pred_train_combM,combineddataMtrain_data$Result)
prob_test_combM <- predict(combineddataM_logreg,combineddataMtest_data, type = "response")

preds_test_combM <- ifelse(prob_test_combM > 0.65, "1", "0")

table(preds_test_combM)

test_data_labs_combM <- combineddataMtest_data$Result

conf_matrix_combM <- table(test_data_labs_combM, preds_test_combM)

print(conf_matrix_combM)

accuracy_combM <- sum(diag(conf_matrix_combM))/sum(conf_matrix_combM)

print(accuracy_combM)

library(caret)
# Using the argument "Positive", we can get the evaluation metrics according to our positive referene level

confusionMatrix(preds_test_combM, combineddataMtest_data$Result, positive = "1")

#####################################################################################

```
#K fold cross validation
```{r}
library(caret)
set.seed(123)
#define train control
train_control_combM <- trainControl(method="cv", number=5)
# fix the parameters of the algorithm

#grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
Knncv_model_combM<- train(Result~., data=combineddataMtrain_data,trControl=train_control_combM,method="glm",preProcess = c("center","scale"))
summary(Knncv_model_combM)
#plot(Knncv_model)
predtrain_cv_combM=predict(Knncv_model_combM,combineddataMtrain_data)
Tabletrain_cv_combM=table(predtrain_cv_combM,combineddataMtrain_data$Result)
print(Tabletrain_cv_combM)


accuracytrain_combM <- sum(diag(Tabletrain_cv_combM))/sum(Tabletrain_cv_combM)
print(accuracytrain_combM)
pred_cv_combM=predict(Knncv_model_combM,combineddataMtest_data)
class(pred_cv_combM)
Table_cv_combM=table(pred_cv_combM,combineddataMtest_data$Result)
print(Table_cv_combM)
accuracy_combM <- sum(diag(Table_cv_combM))/sum(Table_cv_combM)
print(accuracy_combM)
confusionMatrix(pred_cv_combM, combineddataMtest_data$Result, positive = "1")

```
###Decision Trees 
```{r}
library(C50)
c5_tree <- C5.0(Result ~ . ,combineddataMtrain_data)

# Use the rules = T argument if you want to extract rules later from the model

c5_rules <- C5.0(Result~ . ,combineddataMtrain_data, rules = T)
```

### Variable Importance in trees
```{r}
C5imp(c5_tree, metric = "usage")
```
### Rules from trees
```{r}
summary(c5_rules)
```
###plot 
```{r}
plot(c5_tree)
```
### Predictions on the test data
```{r}
preds_trainDT<-predict(c5_tree,combineddataMtrain_data)
confusionMatrix(preds_trainDT, combineddataMtrain_data$Result)
preds <- predict(c5_tree, combineddataMtest_data)
####confusionMatrix()
library(caret)
library(rattle)
confusionMatrix(preds, combineddataMtest_data$Result)
```

####SVM
```{r}
library(e1071)
tunedsvmmodel<- tune.svm(Result ~., data = combineddataMtrain_data, kernel='linear',gamma=10^(-6 :-1),cost=10^(-1:1),probability=T)
summary(tunedsvmmodel)
tunedsvmmodel$best.parameters
#rm(fitted_probM,fittedValues,test_probM,predValues)
fitted_probM =  predict(tunedsvmmodel$best.model, combineddataMtrain_data[-1], decision.values = TRUE, probability = TRUE)
classNames=c('1','0')
fittedValues=classNames[apply(attr(fitted_probM, "probabilities"),1,function(x) which(x==max(x)))]
##########
test_probM =  predict(tunedsvmmodel$best.model, combineddataMtest_data[-1], decision.values = TRUE, probability = TRUE)
#head(attr(test_probM,"probabilities"))
predValues=factor(test_probM,levels=c("0","1"))
print('Confusion Matrix on Train Data.')
confusionMatrix(combineddataMtrain_data$Result,fittedValues)
print('Confusion Matrix on Test Data.')
library(caret)
confusionMatrix(combineddataMtest_data$Result,predValues)
par(mfrow=c(2,2))
plot(fittedValues)
plot(predValues)

```

####random forest
```{r}
library(randomForest)
rf.M = randomForest(Result~ ., data=combineddataMtrain_data,ntree=10)
rf.pred1 <- predict(rf.M, combineddataMtrain_data)
rf.pred2 <- predict(rf.M, combineddataMtest_data)
library(caret)
confusionMatrix(rf.pred1,combineddataMtrain_data$Result,positive = "1")
confusionMatrix(rf.pred2,combineddataMtest_data$Result,positive ="1" )
```
#plot the RF tree 
```{r}
library(multcomp)
library("party")
x <- ctree(Result ~ ., data=combineddata_M)
plot(rf.M)

varImpPlot(rf.M)

```